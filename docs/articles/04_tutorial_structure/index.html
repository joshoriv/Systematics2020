<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Tutorial Structure • Systematics2020</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><meta property="og:title" content="Tutorial Structure">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">Systematics2020</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/00_Basics_of_R/index.html">R Reminder</a>
    </li>
    <li>
      <a href="../../articles/01_homology_assignment/index.html">Morphological Homology Assignment</a>
    </li>
    <li>
      <a href="../../articles/02_molecular_homology/index.html">Molecular Homology Assignment</a>
    </li>
    <li>
      <a href="../../articles/03_ParsimonyAndModels/index.html">PAUP, parsimony, and nucletoide substitution models</a>
    </li>
    <li>
      <a href="../../articles/04_tutorial_structure/index.html">Tutorial Structure</a>
    </li>
    <li>
      <a href="../../articles/05_pandemic_week/index.html">Pandemic Week</a>
    </li>
    <li>
      <a href="../../articles/06_models/index.html">Nucleotide substitution models</a>
    </li>
    <li>
      <a href="../../articles/Homework/Homework3.html">Homework3.Rmd</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Tutorial Structure</h1>
            
      
      
      <div class="hidden name"><code>index.Rmd</code></div>

    </div>

    
    
<div id="using-rstudio" class="section level1">
<h1 class="hasAnchor">
<a href="#using-rstudio" class="anchor"></a>Using RStudio</h1>
<p>Many RevBayes users may want to use RevBayes through RStudio, a popular graphical interface for R. R is a fairly common computing language in biology. In this section of the tutorial, we will focus on running RevBayes from RStudio. Once you’ve followed the RStudio instructions on the installs page, you can run use Rev language as you would in a standard RMarkown document.</p>
<p>RStudio has an interface with four panes: the editor (upper left), console (lower left), environment (upper right), and files (lower right). This can be seen below</p>
<p><img src="figures/RStudio.png"></p>
<p>In the upper-right hand panel, start a new RMarkdown document. RMarkdown is used via “chunks”, or lines of code. In a new RMarkdown notebook, the first cell is shown in {% ref markdownsetup}</p>
<p><img src="figures/RSetup.png"></p>
<p>We will edit this cell to add two variables, the root directory and the engine.path. The working directory tells R what directory you want to work in. We will set this equal to the directory to which we downloaded the data and scripts. For example, if the directory structure if as shown in figure {% ref example %}, this line will be <code>knitr::opts_chunk$set(root.dir = "~/projects/tutorials/tutorial_structure/)</code> on Mac and Linux. On PC, it will be <code><a href="https://rdrr.io/r/base/getwd.html">setwd("c:\\april\\tutorials\\tutorial_structure")</a></code>.</p>
<p>The engine.path is where RevBayes is on your computer. If my copy of RevBayes is in a directory called “software” in my user home, my enine path will be <code>knitr::opts_chunk$set(engine.pat="~/software/rb")</code> on Mac and Linux or <code>knitr::opts_chunk$set(engine.pat="c:\\april\\software\\rb")</code> on PC.</p>
<p>The appearance of the setup cell can be seen below.</p>
<p><img src="figures/CompletedSetup.png"></p>
<p>Run this chunk by clicking the green arrow in the upper-left hand corner of the cell.</p>
<p>Next, you will note other RMarkdown chunks in the document. By changing the <code>r</code> in the curly braces to <code>rb</code>, we can run RevBayes in a markdown document. By choosing <code>r</code> or <code>rb</code>, we cna use both the R programming language and RevBayes in the same notebook.</p>
<p><img src="figures/RbMarkdown.png"></p>
<p>Create a new RevMarkdown chunk as show above. Into this cell, we will paste the following code:</p>
<pre class="revbayes"><code>variable &lt;- "Hi there! Welcome to RevBayes! I am now going to read in some test data."

variable</code></pre>
<pre><code>## Hi there! Welcome to RevBayes! I am now going to read in some test data.</code></pre>
<p>Run it by clicking the green arrow. If this execute correctly, you will receive the message.</p>
<pre><code>   Processing file "scripts/test.Rev"
   Hi there! Welcome to RevBayes! I am now going to read in some test data.</code></pre>
<p>Now, we’ll actually read in the data. Start a new Markdown chunk and enter:</p>
<pre><code>molecular &lt;- readDiscreteCharacterData("data/example_file.nex")

success &lt;- "Congratulations, you set up your scripts and code directories correctly."

success
q()</code></pre>
<p>If this executes correctly, you will receive the output:</p>
<pre><code>   Successfully read one character matrix from file 'data/primates_and_galeopterus_cytb.nex'
   Congratulations, you set up your scripts and code directories correctly.</code></pre>
<p>If you have made a mistake and need to erase previous output, you can add a flag to the markdown cells that refreshes previously-entered commands, as shown in figure {% ref mistake %}.</p>
<p><img src="figures/freshchunkrb.png"></p>
<p>Lastly, the entire document can be <code>knitted</code> to an output file. At the top of your screen, there is a button labeled Knit. This allows you to run all of your code and save the text, output, and figures to html, PDF, and other formats.</p>
</div>
<div id="introduction-to-mcmc-using-revbayes" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction-to-mcmc-using-revbayes" class="anchor"></a>Introduction to MCMC using RevBayes</h1>
<p>Adapted from Wade Dismukes, Tracy A. Heath, Walker Pett</p>
<p>This tutorial is intended to provide a introduction to the basics of Markov chain Monte Caro (MCMC) using the Metropolis-Hastings algorithm. This will provide a brief introduction to MCMC moves as well as prior distributions. We begin with a simple example of estimating the probability distribution of an archer’s ability to shoot at a target, and the distance those arrows land from the center. We will simulate data using this example and attempt to estimate the posterior distribution using a variety of MCMC moves.</p>
<div id="modeling-an-archers-shots-on-a-target" class="section level2">
<h2 class="hasAnchor">
<a href="#modeling-an-archers-shots-on-a-target" class="anchor"></a>Modeling an Archer’s Shots on a Target</h2>
<p>{% figure target %} <img src="figures/target.png" width="200"><br>
{% figcaption %} <em>Representation of the archery data used in this tutorial.Each yellow dot represents the position of an arrow shot by an archer.The distance of each arrow from the the center of the target is assumed to be exponentially distributed with mean <span class="math inline">\(\mu\)</span>.</em> {% endfigcaption %} {% endfigure %}</p>
<p>We’ll begin our exploration of Bayesian inference with a simple archery model. For this model, there is an unknown archer shooting <span class="math inline">\(n\)</span> arrows at a target (see {% ref target %}). The distance <span class="math inline">\(d\)</span> of each arrow from the target’s center is measured. Let’s assume that the distance of each arrow from the bullseye follows an exponential distribution—<em>i.e.,</em> <span class="math inline">\(d\sim\mbox{Exp}(\mu^{-1})\)</span>. This implies the archer has an inherent ability to shoot arrows at an average distance <span class="math inline">\(\mu\)</span>. Then, the probability density of each arrow distance <span class="math inline">\(d_i\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
P(d_i \mid \mu) = \frac{1}{\mu} e^{-d_i/\mu}.
\end{aligned}
\]</span></p>
<p>Simple intuition suggests that, given that we observe <span class="math inline">\(n\)</span> arrows, a good estimate of <span class="math inline">\(\mu\)</span> is the average of all the arrow distances <span class="math inline">\(\bar d = \frac{1}{n}\sum_{i=1}^n d_i\)</span>. Indeed this is the maximum likelihood estimate! In fact, given <span class="math inline">\(n\)</span> arrows whose distances follow an exponential distribution, it turns out that the observed average <span class="math inline">\(\bar d\)</span> follows a gamma distribution, with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(n/\mu\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
P(\bar d \mid \mu,n) = \frac{(n/\mu)^n}{\Gamma(n)} {\bar d}\,^{n-1}e^{-n\bar d /\mu}.
\end{aligned}
\]</span></p>
<p>In this case, the average <span class="math inline">\(\bar d\)</span> acts as a <em>sufficient statistic</em> for <span class="math inline">\(\mu\)</span>. This means that it tells just as much about <span class="math inline">\(\mu\)</span> as the collection of individual arrow distances. Therefore, we will use a Gamma<span class="math inline">\((n, n/\mu)\)</span> distribution on <span class="math inline">\(\bar d\)</span> as the likelihood of our data.</p>
<p>From Bayes’ theorem, the <em>posterior distribution</em> of <span class="math inline">\(\mu\)</span> given <span class="math inline">\(\bar d\)</span>, <span class="math inline">\(P(\mu \mid \bar d)\)</span>, is:</p>
<p><span class="math display">\[
\begin{aligned}
P\left(\mu \mid \bar d\right) = \frac{P\left(\bar d \mid \mu\right) \times P\left(\mu\right)}{P\left(\bar d\right)}
\end{aligned}
\]</span></p>
<p>Where <span class="math inline">\(P(\mu \mid \bar d)\)</span> is our posterior distribution, <span class="math inline">\(P(\bar d \mid \mu)\)</span> is our likelihood or data distribution, <span class="math inline">\(P(\mu)\)</span> is our prior distribution, and <span class="math inline">\(P(\bar d)\)</span> is our marginal likelihood. The take-home message here is that, if we’re interested in doing Bayesian inference for the archery model, we need to specify a <em>likelihood function</em> and a <em>prior distribution</em> for <span class="math inline">\(\mu\)</span>. In virtually all practical cases, we cannot compute the posterior distribution directly and instead use numerical procedures, such as a Markov chain Monte Carlo (MCMC) algorithm. Therefore, we will also have to write a MCMC algorithm that samples parameter values in the frequency of their posterior probability.</p>
<p>We’ll use a simple exponential distribution as a prior on the parameter of the model, <span class="math inline">\(\mu\)</span>. The <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</a> has one parameter <span class="math inline">\(\alpha\)</span> representing our prior belief about the mean arrow distance {% ref exponential %}. Different choices for <span class="math inline">\(\alpha\)</span> represent different prior beliefs.</p>
<p>{% figure exponential %} <img src="figures/exp.png"> {% figcaption %} <em>Exponential distribution with one parameter <span class="math inline">\(\alpha\)</span>. This distribution is used as a prior distribution on the average arrow distance <span class="math inline">\(\mu\)</span>.Here we show different curves for the exponential distribution when using different parameters.</em> {% endfigcaption %} {% endfigure %}</p>
<p>{% ref archery_model %} shows the graphical model for the archery model. This nicely visualizes the dependency structure in the model. We see that the parameter <span class="math inline">\(\alpha\)</span> is drawn in a solid square, representing that this variable is constant (<em>i.e.,</em> it takes a “known” value). Following the graph in {% ref archery_model %}, we see an arrow connecting <span class="math inline">\(\alpha\)</span> and the variable <span class="math inline">\(\mu\)</span>. That simply means that <span class="math inline">\(\mu\)</span> depends on <span class="math inline">\(\alpha\)</span>. More specifically, <span class="math inline">\(\mu\)</span> is a stochastic variable (shown as a solid circle) that is drawn from an exponential distribution with parameter <span class="math inline">\(\alpha\)</span>. Another constant variable, <span class="math inline">\(n\)</span>, represents the number of shots taken by the archer. Finally, we have the observed data <span class="math inline">\(\bar d\)</span> which is drawn from a gamma distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(n\)</span>, as can be seen by the arrows pointing from those parameters to <span class="math inline">\(d\)</span>. Furthermore, the solid circle of <span class="math inline">\(\bar d\)</span> is shaded which means that the variable has data attached to it.</p>
<p>{% figure archery_model %} <img src="figures/archery_graphical_model.png" width="200"><br>
{% figcaption %} <em>Graphical model for the archery model.</em> {% endfigcaption %} {% endfigure %}</p>
</div>
</div>
<div id="writing-mcmc-from-scratch" class="section level1">
<h1 class="hasAnchor">
<a href="#writing-mcmc-from-scratch" class="anchor"></a># Writing MCMC from Scratch</h1>
<div id="tutorial-format" class="section level3">
<h3 class="hasAnchor">
<a href="#tutorial-format" class="anchor"></a>Tutorial Format</h3>
<p>This tutorial follows a specific format for issuing instructions and information.</p>
<p>The boxed instructions guide you to complete tasks that are not part of the RevBayes syntax, but rather direct you to create directories or files or similar.</p>
<p>Information describing the commands and instructions will be written in paragraph-form before or after they are issued.</p>
<p>All command-line text, including all <code>Rev</code> syntax, are given in <code>monotype font</code>. Furthermore, blocks of <code>Rev</code> code that are needed to build the model, specify the analysis, or execute the run are given in separate shaded boxes. For example, we will instruct you to create a new variable called <code>n</code> that is equal to <code>10</code> using the <code>=</code> operator like this:</p>
<pre><code>n = 10</code></pre>
</div>
<div id="create-your-script-file" class="section level2">
<h2 class="hasAnchor">
<a href="#create-your-script-file" class="anchor"></a>Create Your Script File</h2>
<p>Make yourself familiar with the example script called <a href="https://raw.githubusercontent.com/revbayes/revbayes_tutorial/master/RB_MCMC_Archery_Tutorial/archery_MH.Rev"><code>archery_MH.Rev</code></a> which shows the code for the following sections.</p>
<p>Name the script file <code>my_archery_MH.Rev</code> or anything you’d like.</p>
</div>
<div id="the-metropolis-hastings-algorithm" class="section level2">
<h2 class="hasAnchor">
<a href="#the-metropolis-hastings-algorithm" class="anchor"></a>The Metropolis-Hastings Algorithm</h2>
<p>Though RevBayes implements efficient and easy-to-use Markov chain Monte Carlo (MCMC) algorithms, we’ll begin by writing one ourselves to gain a better understanding of the moving parts. The Metropolis-Hastings MCMC algorithm {% cite Metropolis1953 %} {% cite Hastings1970 %} proceeds as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Generate initial values for the parameters of the model (in this case, <span class="math inline">\(\mu\)</span>).</p></li>
<li><p>Propose a new value (which we’ll call <span class="math inline">\(\mu^\prime\)</span>) for some parameters of the model, (possibly) based on their current values</p></li>
<li>
<p>Calculate the acceptance probability, <span class="math inline">\(R\)</span>, according to:</p>
<p><span class="math display">\[\begin{aligned}
R &amp;= \text{min}\left\{1, \frac{P(\bar d \mid \mu^\prime)}{P(\bar d \mid \mu)} \times \frac{P(\mu^\prime)}{P(\mu)} \times \frac{q(\mu)}{q(\mu^\prime)} \right\}\\
  &amp;= \text{min}\left\{1, \text{likelihood ratio} \times \text{prior ratio} \times \text{proposal ratio} \right\},
\end{aligned}\]</span></p>
<p>where the proposal ratio (also called the Hastings ratio) ensures the correct target density, even if the move is biased.</p>
</li>
<li>
<p>Generate a uniform random number <span class="math inline">\(u\)</span> between 1 and 0.</p>
<dl>
<dt>if <span class="math inline">\(u&lt;R\)</span>:</dt>
<dd>
<p>then accept the move and set <span class="math inline">\(\mu = \mu^\prime\)</span>.</p>
</dd>
<dt>else (if <span class="math inline">\(u \geq R\)</span>):</dt>
<dd>
<p>the value of <span class="math inline">\(\mu\)</span> does not change and the move is rejected: <span class="math inline">\(\mu = \mu\)</span>.</p>
</dd>
</dl>
</li>
<li><p>Record the values of the parameters.</p></li>
<li><p>Return to step 2 many, many times, keeping track of the value of <span class="math inline">\(\mu\)</span>.</p></li>
</ol>
</div>
<div id="reading-in-the-data" class="section level2">
<h2 class="hasAnchor">
<a href="#reading-in-the-data" class="anchor"></a>## Reading in the data</h2>
<p>Since we do not have access to archery data, we will simulate the the shots of our archer using the simulation tools in RevBayes. By simulating the data, we can also evaluate how well our moves and prior model perform—<em>i.e.,</em> how robust and accurate our estimators are. After completing this exercise, feel free to repeat it and alter the true values to see how they influence the posterior distribution.</p>
<pre class="revbayes"><code>    # Simulate some data (i.e. shoot some arrows)
    # First we need the number of arrows to shoot
    n = 10
    # Then we need a true mean distance
    mu_true = 1
    # Simulate the observed mean distance of the arrows shot from a gamma distribution
    arrow_mean = rgamma(1, n, n/mu_true)[1]</code></pre>
<p>The Rev code above uses the <code><a href="https://rdrr.io/r/stats/GammaDist.html">rgamma()</a></code> function to simulate a single observed <code>arrow_mean</code> from <span class="math inline">\(n=10\)</span> arrows shot on target. The <code>[1]</code> following the <code><a href="https://rdrr.io/r/stats/GammaDist.html">rgamma()</a></code> function is needed because this function always returns a <em>vector</em> even when we only request a single value. Thus, in order to treat <code>arrow_mean</code> as a single value, we have to request the first element of the vector returned by that function.</p>
</div>
<div id="initializing-the-markov-chain" class="section level2">
<h2 class="hasAnchor">
<a href="#initializing-the-markov-chain" class="anchor"></a>Initializing the Markov chain</h2>
<p>We have to start the MCMC off with some initial values for all of the parameters. One way to do this is to randomly draw values of the parameters (just <span class="math inline">\(\mu\)</span>, in this case) from the prior distribution. We’ll assume a simple exponential prior distribution; that is, one with <span class="math inline">\(\alpha = 1\)</span>.</p>
<pre class="revbayes"><code># Initialize the chain with some starting value
alpha = 1.0
mu = rexp(1, alpha)[1]</code></pre>
<div id="likelihood-function" class="section level3">
<h3 class="hasAnchor">
<a href="#likelihood-function" class="anchor"></a>Likelihood function</h3>
<p>Next we will specify the likelihood function, which will compute the probability of our data given the prior model. We use the gamma distribution for the likelihood. Since the likelihood is defined only for values of <span class="math inline">\(\mu\)</span> greater than 0, we return a likelihood of 0.0 if <span class="math inline">\(\mu\)</span> is negative:</p>
<pre class="revbayes"><code># Define the likelihood function on the mean
function likelihood(mu){
    if(mu &lt; 0.0)
        return 0.0
    return dgamma(arrow_mean, n, n/mu, log=false)
}</code></pre>
<p>In <code>Rev</code>, we can create a <em>user-defined function</em> using the <code>function</code> keyword. In our function definition above, <code>likelihood()</code> takes a single value as an argument that is expected to be the mean (<span class="math inline">\(\mu\)</span>) value. All other parameters in our function are expected to be defined before <code>likelihood()</code> is called.</p>
</div>
<div id="prior-distribution" class="section level3">
<h3 class="hasAnchor">
<a href="#prior-distribution" class="anchor"></a>Prior distribution</h3>
<p>Similarly, we need to define a function for the prior distribution. Here, we use the exponential probability distribution for the prior on <span class="math inline">\(\mu\)</span>:</p>
<pre class="revbayes"><code># Define the prior function on the mean
function prior(mu){
    if(mu &lt; 0.0)
        return 0.0
    return dexp(mu, alpha, log=false)
}</code></pre>
</div>
<div id="monitoring-parameter-values" class="section level3">
<h3 class="hasAnchor">
<a href="#monitoring-parameter-values" class="anchor"></a>Monitoring parameter values</h3>
<p>Additionally, we are going to monitor, <em>i.e.</em> store, parameter values into a file during the MCMC simulation. For this file we need to write the column headers in the first line of our output file, which we will name <code>archery_MH.log</code> (you may have to change the newline characters from <code>\n</code> to <code>\r\n</code> if you’re using a Windows operating system.):</p>
<pre class="revbayes"><code><a href="https://rdrr.io/r/base/write.html"># Prepare a file to log our samples
write("iteration","mu","\n",file="archery_MH.log")
write(0,mu,"\n",file="archery_MH.log",append=TRUE)</a></code></pre>
<p>We’ll also monitor the parameter values to the screen, so let’s print the initial values:</p>
<pre class="revbayes"><code><a href="https://rdrr.io/r/base/print.html"># Print the initial values to the screen
print("iteration","mu")
print(0,mu)</a></code></pre>
<pre><code>## [1] iteration    mu
## [2] 0    3.214789</code></pre>
</div>
</div>
<div id="writing-the-metropolis-hastings-algorithm" class="section level2">
<h2 class="hasAnchor">
<a href="#writing-the-metropolis-hastings-algorithm" class="anchor"></a>Writing the Metropolis-Hastings Algorithm</h2>
<p>At long last, we can write our MCMC algorithm. First, we define how often we print to file (<em>i.e.,</em> monitor); this is called thinning if we do not choose to save every value of our parameter to file. If we set the variable <code>printgen=1</code>, then we will store the parameter values at every iteration; if we instead choose <code>printgen=10</code>, then we’ll only save the values every <span class="math inline">\(10^{th}\)</span> step in our Markov chain.</p>
<pre class="revbayes"><code># Write the MH algorithm    
printgen = 10</code></pre>
<p>We will repeat this resampling procedure many times and iterate the MCMC using a <code>for</code> loop (<em>e.g.,</em> step 6 in <a href="#sect:MH_algorithm">The Metropolis-Hastings Algorithm Section</a>). We will start this part by defining the number of iterations for our MCMC ( <code>reps = 10000</code>), and writing the first line of our ‘for‘ loop. We’ll also define a variable <code>delta</code> (explained momentarily).</p>
<pre class="revbayes"><code>reps = 10000
delta = 1
for(rep in 1:reps){</code></pre>
<pre><code>## [1] Processing of file ".rb4588578e11f.Rev" completed
## [2] &gt;</code></pre>
<p>In <code>Rev</code>, the contents of every <code>for</code> loop must be enclosed within a set of ‘curly braces’ . Our loop will not be complete until we finish it and add the closing brace. Additionally, it is good style to make our loops readable by indenting the contents within the curly braces. We recommend that you use 4 spaces to represent these indents.</p>
<p>For our MCMC algorithm, the first thing we do is generate a new value of <span class="math inline">\(\mu^\prime\)</span> to evaluate (step 2 of the <a href="#sect:MH_algorithm">The Metropolis-Hastings Algorithm Section</a>). We’ll propose a new value of <span class="math inline">\(\mu\)</span> by drawing a random number from a uniform window and then adding this random number to the current value (<em>i.e.</em> centered on the previous value). The value of <code>delta</code> defines the width of the uniform window from which we draw new values. Thus, if <code>delta</code> is large, then the proposed values are more likely to be very different from the current value of <span class="math inline">\(\mu\)</span>. Conversely, if <code>delta</code> is small, then the proposed values are more likely to be very close to the current value of <span class="math inline">\(\mu\)</span>. By changing the value of <code>delta</code> we can tune the behavior of the proposal, and therefore <code>delta</code> is called a <em>tuning parameter</em>.</p>
<pre class="revbayes"><code># Propose a new value of p
mu_prime &lt;- mu + runif(n=1,-delta,delta)[1]</code></pre>
<p>Next, we compute the proposed likelihood and prior probabilities using the functions we defined above, as well as the acceptance probability, <span class="math inline">\(R\)</span> (step 3 of the <a href="#sect:MH_algorithm">The Metropolis-Hastings Algorithm Section</a>):</p>
<pre class="revbayes"><code># Compute the acceptance probability
R = ( likelihood(mu_prime)/likelihood(mu) ) * ( prior(mu_prime)/prior(mu) )</code></pre>
<p>Then, we accept the proposal with probability <span class="math inline">\(R\)</span> and reject otherwise (step 4 of the <a href="#sect:MH_algorithm">The Metropolis-Hastings Algorithm Section</a>):</p>
<pre class="revbayes"><code># Accept or reject the proposal
u = runif(1,0,1)[1]

if(u &lt; R){
# Accept the proposal
    mu = mu_prime 
}</code></pre>
<p>Finally, we store the current value of <span class="math inline">\(\mu\)</span> in our log file (step 5 of the the <a href="#sect:MH_algorithm">The Metropolis-Hastings Algorithm Section</a>). Here, we actually check if we want to store the value during this iteration.</p>
<pre class="revbayes"><code>if ( (rep % printgen) == 0 ) {
    # Write the samples to a file
    write(rep,mu,"\n",file="archery_MH.log",append=TRUE)
    # Print the samples to the screen
    print(rep,mu)
}</code></pre>
<p>and close the for loop</p>
<pre><code>}</code></pre>
</div>
<div id="exercise-1" class="section level2">
<h2 class="hasAnchor">
<a href="#exercise-1" class="anchor"></a>Exercise 1</h2>
<ol style="list-style-type: decimal">
<li>The <code>.log</code> file will contain samples from the posterior distribution of the model. Open the file in <code>Tracer</code> to learn about various features of the posterior distribution, for example: the posterior mean or the 95% credible interval.</li>
</ol>
<p>Pretty awesome, right?</p>
<p>Below we show an example of the obtained output in <code>Tracer</code>. Specifically, {% ref mcmc-trace %} shows the sample trace (left) and the estimated posterior distribution of <span class="math inline">\(\mu\)</span> (right). There are other parameters, such as the posterior mean and the 95% HPD (highest posterior density) interval, that you can obtain from <code>Tracer</code>.</p>
<p>{% figure mcmc-trace %} <img src="figures/archery_MCMC_Trace.png" width="45%"><img src="figures/archery_MCMC_distribution.png" width="45%"> {% figcaption %} The <em>Trace</em> of sample from an MCMC simulation. Right: The approximated posterior probability distribution for <span class="math inline">\(\mu\)</span>. {% endfigcaption %} {% endfigure %}</p>
</div>
<div id="more-on-moves-tuning-and-weights" class="section level2">
<h2 class="hasAnchor">
<a href="#more-on-moves-tuning-and-weights" class="anchor"></a>More on Moves: Tuning and Weights</h2>
<p>In the previous example we hard coded a single move updating the variable <span class="math inline">\(\mu\)</span> by drawing a new value from a sliding window. There are other ways how to propose new values; some of which are more efficient than others.</p>
<p>First, let us rewrite the MCMC loop so that instead we call a function, which we name <code>move_slide</code> for simplicity, that performs the move:</p>
</div>
<div id="slide-move" class="section level2">
<h2 class="hasAnchor">
<a href="#slide-move" class="anchor"></a>Slide move</h2>
<p>{:.subsection}</p>
<p>Now we need to actually write the <code>move_slide</code> function. We mostly just copy the code we had before into a dedicated function</p>
<p>There are a few things to consider in the function <code>move_slide</code>. First, we do not have a return value because the move simply changes the variable <span class="math inline">\(\mu\)</span> if the move is accepted. Second, in addition to the tuning parameter <code>delta</code>, we expect an argument called <code>weight</code> which will tell us how often we want to use this move. Otherwise, this function does exactly the same what was inside the for loop previously.</p>
<p>(Note that you need to define this function before the for loop in your script).</p>
<p>Experiment with different values for <code>delta</code> and see how the effective sample size (ESS) changes.</p>
<p>There is, <em>a priori</em>, no good method for knowing what values of <code>delta</code> are most efficient. However, there are some algorithms implemented in RevBayes, called <em>auto-tuning</em>, that will estimate good values for <code>delta</code>.</p>
<div id="scaling-move" class="section level3">
<h3 class="hasAnchor">
<a href="#scaling-move" class="anchor"></a>Scaling move</h3>
<p>As another move we will write a scaling move. The scaling move proposes an update by drawing a random number from a <span class="math inline">\(Uniform(-0.5,0.5)\)</span> distribution, exponentiating the random number, and then multiplying this scaling factor by the current value. An interesting feature of this move is that it is not symmetrical and thus needs a Hastings ratio (this is the same as the proposal ratio given in Section [sect:MH_algorithm]). The Hastings ratio is rather trivial in this case, and one only needs to multiply the acceptance rate by the scaling factor.</p>
<p>As before, this move has a tuning parameter called <code>lambda</code>.</p>
<p>The sliding-window and scaling moves are very common and popular moves in RevBayes. The code examples here are actually showing the exact same equation as implemented internally. It will be very useful for you to understand these moves.</p>
</div>
<div id="exercise-2" class="section level3">
<h3 class="hasAnchor">
<a href="#exercise-2" class="anchor"></a>Exercise 2</h3>
<ol style="list-style-type: decimal">
<li><p>Rewrite your previous script to include these two different moves, and re-run the script to estimate the posterior distribution of <span class="math inline">\(\mu\)</span> again.</p></li>
<li><p>Use only a single move and set <code>printgen=1</code>. Which move has the best ESS?</p></li>
</ol>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">

        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#using-rstudio">Using RStudio</a></li>
      <li>
<a href="#introduction-to-mcmc-using-revbayes">Introduction to MCMC using RevBayes</a><ul class="nav nav-pills nav-stacked">
<li><a href="#modeling-an-archers-shots-on-a-target">Modeling an Archer’s Shots on a Target</a></li>
      </ul>
</li>
      <li>
<a href="#writing-mcmc-from-scratch"># Writing MCMC from Scratch</a><ul class="nav nav-pills nav-stacked">
<li><a href="#create-your-script-file">Create Your Script File</a></li>
      <li><a href="#the-metropolis-hastings-algorithm">The Metropolis-Hastings Algorithm</a></li>
      <li><a href="#reading-in-the-data">## Reading in the data</a></li>
      <li><a href="#initializing-the-markov-chain">Initializing the Markov chain</a></li>
      <li><a href="#writing-the-metropolis-hastings-algorithm">Writing the Metropolis-Hastings Algorithm</a></li>
      <li><a href="#exercise-1">Exercise 1</a></li>
      <li><a href="#more-on-moves-tuning-and-weights">More on Moves: Tuning and Weights</a></li>
      <li><a href="#slide-move">Slide move</a></li>
      </ul>
</li>
      </ul>
</div>
      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by The package maintainer.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
